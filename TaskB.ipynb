{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./OLID/olid-training-v1.0.tsv\", sep=\"\\t\")\n",
    "df['subtask_b'][df['subtask_b']=='UNT']=0\n",
    "df['subtask_b'][df['subtask_b']=='TIN']=1\n",
    "df = df[pd.notnull(df['subtask_b'])]\n",
    "df['subtask_c']=df['tweet']\n",
    "df['tweet']=df['subtask_b']\n",
    "df=df.drop(['subtask_b'],axis=1)\n",
    "df=df.rename({\"tweet\": \"label\",\"subtask_a\":\"alpha\",\"subtask_c\":\"text\"},axis=1) \n",
    "df['alpha']='a'\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"./OLID/train.tsv\",sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import os\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification, BertForPreTraining\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tools import *\n",
    "import convert_examples_to_features\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./OLID\"\n",
    "\n",
    "\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "\n",
    "\n",
    "TASK_NAME = 'taskb'\n",
    "\n",
    "\n",
    "OUTPUT_DIR = f'outputs/{TASK_NAME}/'\n",
    "\n",
    "\n",
    "REPORTS_DIR = f'reports/{TASK_NAME}_evaluation_report/'\n",
    "\n",
    "\n",
    "CACHE_DIR = './uncased_bert/'\n",
    "\n",
    "\n",
    "MAX_SEQ_LENGTH = 50\n",
    "\n",
    "TRAIN_BATCH_SIZE = 40\n",
    "TEST_BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "RANDOM_SEED = 42\n",
    "WARMUP_PROPORTION = 0.1\n",
    "#OUTPUT_MODE = 'classification'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji \n",
    "import wordsegment\n",
    "import codecs\n",
    "wordsegment.load()\n",
    "\n",
    "fp = codecs.open(\"./OLID/olid-training-v1.0.tsv\", \"r\", encoding='utf-8', errors='ignore')\n",
    "data= fp.read()\n",
    "examples=str(data)\n",
    "examples = emoji.demojize(examples)\n",
    "\n",
    "train = examples.split('\\n')[:int(0.8*len(examples.split('\\n')))]\n",
    "test = examples.split('\\n')[int(0.8*len(examples.split('\\n'))):]\n",
    "train_examples = []\n",
    "for i in range(1,len(train) - 1):\n",
    "  \n",
    "    x = train[i].split('\\t')\n",
    "\n",
    "    x[1] = wordsegment.segment(x[1])\n",
    "    s = \"\"\n",
    "    for j in x[1]:\n",
    "        s = s + \" \" + j\n",
    "        x[1] = s\n",
    "    \n",
    "    print(x)\n",
    "    train_examples.append(x)\n",
    "\n",
    "test_examples = []\n",
    "for i in range(1,len(test) - 1):\n",
    "    \n",
    "    x = test[i].split('\\t')\n",
    "\n",
    "    x[1] = wordsegment.segment(x[1])\n",
    "    s = \"\"\n",
    "    for j in x[1]:\n",
    "        s = s + \" \" + j\n",
    "        x[1] = s\n",
    "    \n",
    "    print(x)\n",
    "    test_examples.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BinaryClassificationProcessor()\n",
    "\n",
    "train_examples_len = len(train_examples)\n",
    "\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "\n",
    "num_train_optimization_steps = int(train_examples_len / TRAIN_BATCH_SIZE) * NUM_TRAIN_EPOCHS\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased','./uncased_bert/')\n",
    "\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "train_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer) for example in train_examples]\n",
    "\n",
    "\n",
    "\n",
    "test_examples_len = len(test_examples)\n",
    "test_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH*2, tokenizer) for example in test_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_count=cpu_count()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(process_count) as p:\n",
    "    train_features = list(p.imap(convert_examples_to_features.convert_example_to_feature, train_examples_for_processing))\n",
    "    \n",
    "with Pool(process_count) as p:\n",
    "    test_features = list(p.imap(convert_examples_to_features.convert_example_to_feature, test_examples_for_processing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(BERT_MODEL,cache_dir='./uncased_bert')\n",
    "bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.layer1=nn.Sequential(nn.Conv2d(768, 200, kernel_size=1, padding=0), nn.Softmax(dim=3))\n",
    "        self.layer2=nn.Sequential(nn.Conv2d(768, 200, kernel_size=2, padding=1), nn.Softmax(dim=3))\n",
    "        self.layer3=nn.Sequential(nn.Conv2d(768, 200, kernel_size=3, padding=1), nn.Softmax(dim=3))\n",
    "        self.layer4=nn.Sequential(nn.Conv2d(768, 200, kernel_size=4, padding=2), nn.Softmax(dim=3))\n",
    "        \n",
    "        #self.dropout1=nn.Dropout(0.1)\n",
    "        self.lin1=nn.Sequential(nn.Linear(800,128),nn.Tanh())\n",
    "        #self.dropout2=nn.Dropout(0.2)\n",
    "        self.lin2=nn.Sequential(nn.Linear(128,2),nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.layer1(x)\n",
    "        \n",
    "        out2 = self.layer2(x)[:,:,:-1,:-1]\n",
    "        \n",
    "        out3 = self.layer3(x)\n",
    "        \n",
    "        out4 = self.layer4(x)[:,:,:-1,:-1]\n",
    "       \n",
    "        out5=torch.cat((out1,out2,out3,out4),dim=1)\n",
    "\n",
    "        out5=out5.permute(0,3,2,1)  \n",
    "\n",
    "        out6=self.lin1(out5)\n",
    "\n",
    "        out7=self.lin2(out6)\n",
    "    \n",
    "        out7=out7.permute(0,3,2,1)\n",
    "\n",
    "        out8=torch.sum(out7,dim=3)\n",
    "        \n",
    "        return out8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN=cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(bert.parameters()) + list(CNN.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids_test = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "all_input_mask_test = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids_test = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids_test = torch.tensor([f.label_id for f in test_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(all_input_ids_test, all_input_mask_test, all_segment_ids_test, all_label_ids_test)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "for _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "\n",
    "    for step, batch in enumerate(tqdm_notebook(train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        bert = BertModel.from_pretrained(BERT_MODEL,cache_dir='./uncased_bert')\n",
    "        \n",
    "        \n",
    "        preprocessed = bert(input_ids, segment_ids, input_mask)\n",
    "        inpu=torch.stack(preprocessed[0],axis=0)\n",
    "        inpu=inpu.permute(1,3,2,0)\n",
    "        \n",
    "        CNN=cnn()\n",
    "        outp=CNN(inpu)\n",
    "\n",
    "        x=outp.sum(axis=2)\n",
    "        #         print(outp.sum(axis=1))\n",
    "#         print(outp.sum(axis=1).shape)\n",
    "#         break\n",
    "#     break\n",
    "#         outp[:,1,]\n",
    "#         x=np.array([outp[:,1,:],outp[:,0,:]])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "#             print(label_ids.view(-1))\n",
    "#             print(type(x))\n",
    "#             print(type(label_ids))\n",
    "#             print(x.shape)\n",
    "#             print(type(x.view(-1,2)))\n",
    "        loss = loss_fct(x, label_ids.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        print(\"\\r%f\" % loss, end='')\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "\n",
    "for input_ids_test, input_mask_test, segment_ids_test, label_ids_test in tqdm_notebook(test_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids_test.to(device)\n",
    "    input_mask = input_mask_test.to(device)\n",
    "    segment_ids = segment_ids_test.to(device)\n",
    "    label_ids = label_ids_test.to(device)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bert = BertModel.from_pretrained(BERT_MODEL,cache_dir='./uncased_bert')\n",
    "        preprocessed = bert(input_ids, segment_ids, input_mask)\n",
    "        inpu=torch.stack(preprocessed[0],axis=0)\n",
    "        inpu=inpu.permute(1,3,2,0)\n",
    "        CNN=cnn()\n",
    "        outp=CNN(inpu)\n",
    "        x=outp.sum(axis=2)\n",
    "        \n",
    "        \n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    tmp_eval_loss = loss_fct(x, label_ids.view(-1))\n",
    "    \n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    nb_eval_steps += 1\n",
    "eval_loss = eval_loss / nb_eval_steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
